{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log\n",
    "from math import exp\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as L\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tripadvisor_hotel_reviews.csv')\n",
    "X = df['Review']\n",
    "y = df['Rating']\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "V_size = 10000\n",
    "pad_len = 200\n",
    "embd_dim = 64\n",
    "unit = 64\n",
    "batch_size = 32\n",
    "num_epoch=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(review):\n",
    "    \"\"\" \n",
    "    cleansing of raw data\n",
    "    :param review: a Series of reviews\n",
    "    :return: a Series of cleaned reviews \n",
    "    \"\"\"\n",
    "    # lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # fix negations\n",
    "    review = re.sub(\"n't\", ' not', review)\n",
    "\n",
    "    # remove digits & punctuations\n",
    "    review = re.sub('(\\S*\\d+\\S*)|([^a-z\\s])', ' ', review)\n",
    "\n",
    "    # remove extra white spaces\n",
    "    review = re.sub('\\s+', ' ', review)\n",
    "    review = review.strip()\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review = review.split()\n",
    "    review = \" \".join([lemmatizer.lemmatize(word, pos='v') for word in review])\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_shifting(y):\n",
    "    \"\"\" \n",
    "    encode the ratings to start from 0 \n",
    "    :param y: a Series of ratings ranging from 1 to 5\n",
    "    :return: a Series of ratings ranging from 0 to 4\n",
    "    \"\"\"\n",
    "    mapping = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "    y_new = y.copy()\n",
    "    y_new.replace(mapping, inplace=True)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(X_train, X_test):\n",
    "    \"\"\"\n",
    "    tokenize review texts, turn tokens into lists of sequences\n",
    "    and padding\n",
    "    :param X_train: a Series of reviews for training\n",
    "    :param X_test: a Series of reviews for test\n",
    "    :return V_list: vocabulary list\n",
    "    :return X_seq_train_pad: padded sequences of reviews for training\n",
    "    :return X_seq_test_pad: padded sequences of reviews for test \n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=V_size)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # turn texts to sequences\n",
    "    X_seq_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_seq_test = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # padding\n",
    "    X_seq_train_pad = pad_sequences(X_seq_train, maxlen=pad_len, padding='post', truncating='post')\n",
    "    X_seq_test_pad = pad_sequences(X_seq_test, maxlen=pad_len, padding='post', truncating='post')\n",
    "    \n",
    "    V_list = list(tokenizer.word_index.keys())[:V_size]\n",
    "    \n",
    "    return X_seq_train_pad, X_seq_test_pad, V_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_matrix_gen(X, V):\n",
    "    \"\"\" \n",
    "    Generate a doc-word count matrix \n",
    "    :param X: a Series of reviews \n",
    "    :param V: vocabulary list\n",
    "    :return: doc-word count matrix\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(vocabulary=V)\n",
    "    dw_cnt = vectorizer.fit_transform(X).toarray()\n",
    "    return dw_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_gen(dw_cnt, y):\n",
    "    \"\"\"\n",
    "    generate the likelihood of words in each class \n",
    "    :param dw_cnt: doc-word count matrix\n",
    "    :param y: a Series of ratings\n",
    "    :return: likelihood matrix \n",
    "    \"\"\" \n",
    "    cw_cnt = []\n",
    "    for c in range(5):\n",
    "        cw_cnt.append((dw_cnt[y == c].sum(0) + 1).tolist())    # add-one smoothing\n",
    "\n",
    "    likelihood = []\n",
    "    for c in range(len(cw_cnt)):\n",
    "        temp = []\n",
    "        for pos in range(len(cw_cnt[c])):\n",
    "            S = sum(cw_cnt[c])\n",
    "            temp.append(log(cw_cnt[c][pos] / S))\n",
    "        likelihood.append(temp)\n",
    "\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialNB_train(dw_cnt_train, y_train):\n",
    "    \"\"\"\n",
    "    train multinomial naive Bayes model\n",
    "    :param dw_cnt_train: doc-word count matrix for training\n",
    "    :param y_train: a Series of ratings for training\n",
    "    :return prior: class prior list\n",
    "    :return likelihood: likelihood matrix\n",
    "    \"\"\"\n",
    "    num_doc = len(y_train)\n",
    "    prior = []\n",
    "    for c in range(5):\n",
    "        num_class = y_train.value_counts()[c]\n",
    "        prior.append(log(num_class / num_doc))\n",
    "\n",
    "    likelihood = likelihood_gen(dw_cnt_train, y_train)\n",
    "\n",
    "    return prior, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialNB_test(dw_cnt_test, y_test, prior, likelihood):\n",
    "    \"\"\"\n",
    "    test multinomial naive Bayes model\n",
    "    :param dw_cn_test: doc-word count matrix for test\n",
    "    :param y_test: a Series of ratings for test\n",
    "    :param prior: class prior list\n",
    "    :param likelihood: likelihood matrix\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i in range(len(dw_cnt_test)):\n",
    "        class_prob = []\n",
    "        for c in range(len(prior)):\n",
    "            prob = prior[c]\n",
    "\n",
    "            for pos in range(len(dw_cnt_test[i])):\n",
    "                prob += likelihood[c][pos] * dw_cnt_test[i][pos]\n",
    "            class_prob.append(prob)\n",
    "        class_prob = np.array(class_prob)\n",
    "        y_pred.append(np.argmax(class_prob))    # maximum likelihood estimation\n",
    "    \n",
    "    y_test = y_test.values.tolist()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1 score: \", f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    "    #conf_matrix_plot(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_gen(likelihood, prior, V):\n",
    "    \"\"\"\n",
    "    generate review data using multinomial NB\n",
    "    :param likelihood: likelihood matrix\n",
    "    :param prior: class prior list\n",
    "    :param V: vocabulary list\n",
    "    :return X_gen: a Series of generated reviews \n",
    "    :return y_gen: a Series of generated ratings \n",
    "    \"\"\"\n",
    "    X_gen = []\n",
    "    y_gen = []\n",
    "    prior = [exp(p) for p in prior]\n",
    "    num_class = [int(4000 * p) for p in prior]\n",
    "\n",
    "    for c in range(5):\n",
    "        likelihood_class = [exp(l) for l in likelihood[c]]\n",
    "        for i in range(num_class[c]):\n",
    "            review_len = np.random.poisson(200)    # determine the review length\n",
    "            review=[]\n",
    "            for j in range(review_len):\n",
    "                idx = np.random.multinomial(1, likelihood_class).tolist().index(1)    # determine the words\n",
    "                review.append(V[idx])\n",
    "\n",
    "            X_gen.append(review)\n",
    "            y_gen.append(c)\n",
    "            \n",
    "    X_gen=pd.Series([' '.join(review) for review in X_gen])\n",
    "    y_gen=pd.Series(y_gen)\n",
    "    \n",
    "    return X_gen, y_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embd_matrix_gen(V):\n",
    "    \"\"\"\n",
    "    generate a pretrained embedding matrix\n",
    "    :param V: vocabulary list\n",
    "    :return: pretrained embedding matrix\n",
    "    \"\"\"\n",
    "    # load spaCy language model\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    V_vects = nlp(V_str)\n",
    "    \n",
    "    embd_matrix = np.zeros((V_size, 300))\n",
    "    for i in range(V_size):\n",
    "        if V_vects[i].vector is not None:\n",
    "            embd_matrix[i] = V_vects[i].vector\n",
    "            \n",
    "    return embd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_train(X_seq_train_pad, y_train, embd_matrix, pretrained = False):\n",
    "    \"\"\"\n",
    "    construct a BiLSTM network and train\n",
    "    :param X_seq_train_pad: padded sequences of reviews for training\n",
    "    :param y_train: a Series of ratings for training\n",
    "    :param embd_matrix: pretrained embedding matrix\n",
    "    :param pretrained: train embeddings if False, use pretrained embeddings if True\n",
    "    :return: trained BiLSTM network\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    if not pretrained:\n",
    "        model.add(L.Embedding(V_size, embd_dim))\n",
    "    else:\n",
    "        model.add(L.Embedding(V_size, 300, weights=[embd_matrix], trainable=False))\n",
    "        \n",
    "    model.add(L.Bidirectional(L.LSTM(unit, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#    model.add(L.Flatten())\n",
    "#    model.add(L.Dropout(0.2))\n",
    "#    model.add(L.Dense(1024, activation='relu'))\n",
    "#    model.add(L.Dropout(0.3))\n",
    "#    model.add(L.Dense(512, activation='relu'))\n",
    "#    model.add(L.Dropout(0.3))\n",
    "    model.add(L.Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_seq_train_pad, y_train, batch_size=batch_size, epochs=num_epoch, validation_split=0.15, verbose=2)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_test(model, X_seq_test_pad, y_test):\n",
    "    \"\"\"\n",
    "    test the BiLSTM network\n",
    "    :param model: trained BiLSTM network\n",
    "    :X_seq_test_pad: padded sequences of reviews for test\n",
    "    :y_test: a Series of ratings for test\n",
    "    :return: accuracy score of test\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(model.predict(X_seq_test_pad), axis=-1)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1 score: \", f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    " #   conf_matrix_plot(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_plot(matrix):\n",
    "    \"\"\"\n",
    "    plot the confusion matrix\n",
    "    :param matrix: confusion matrix\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    sn.heatmap(matrix,annot=True,annot_kws={\"size\": 10}) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    process data, train & test models\n",
    "    \"\"\"\n",
    "    # data cleansing\n",
    "    X_clean = X.apply(data_cleansing)\n",
    "    y_new = rate_shifting(y)\n",
    "    \n",
    "    # train-test spliting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y_new, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # tokenization & padding\n",
    "    X_seq_train_pad, X_seq_test_pad, V_list = tokenization(X_train, X_test)\n",
    "    \n",
    "    # pretrained embedding matrix\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    V_str = ' '.join(V_list)\n",
    "    V_vects = nlp(V_str)\n",
    "    embd_matrix = np.zeros((V_size, 300))\n",
    "    for i in range(V_size):\n",
    "        if V_vects[i].vector is not None:\n",
    "            embd_matrix[i] = V_vects[i].vector\n",
    "      \n",
    "    \n",
    "    print(\"Test on real-world data *****************************\")\n",
    "    print(\"##### Multinomial NB #####\")\n",
    "    dw_cnt_train = cnt_matrix_gen(X_train, V_list)\n",
    "    dw_cnt_test = cnt_matrix_gen(X_test, V_list)\n",
    "    prior, likelihood = multinomialNB_train(dw_cnt_train, y_train)\n",
    "    multinomialNB_test(dw_cnt_test, y_test, prior, likelihood)\n",
    "    \n",
    "    print(\"##### Bi-LSTM without pretraiend embeddings #####\")\n",
    "    model1 = BiLSTM_train(X_seq_train_pad, y_train, embd_matrix, pretrained = False)\n",
    "    BiLSTM_test(model1, X_seq_test_pad, y_test)\n",
    "    \n",
    "    print(\"##### Bi-LSTM with pretraiend embeddings #####\")\n",
    "    model2 = BiLSTM_train(X_seq_train_pad, y_train, embd_matrix, pretrained = True)\n",
    "    BiLSTM_test(model2, X_seq_test_pad, y_test)\n",
    "    \n",
    "    \n",
    "    # data generation \n",
    "    X_gen, y_gen = review_gen(likelihood, prior, V_list)\n",
    "    tokenizer.fit_on_texts(X_gen)\n",
    "    X_gen_seq = tokenizer.texts_to_sequences(X_gen)\n",
    "    X_gen_seq_pad = pad_sequences(X_gen_seq, maxlen=pad_len, padding='post', truncating='post')\n",
    "    \n",
    "    print(\"Test on generated data *****************************\")\n",
    "    print('\\n')\n",
    "    print(\"##### Multinomial NB #####\")\n",
    "    dw_cnt_gen=cnt_matrix_gen(X_gen,V_list)\n",
    "    multinomialNB_test(dw_cnt_gen, y_gen, prior, likelihood)\n",
    "    \n",
    "    print(\"##### Bi-LSTM without pretraiend embeddings #####\")\n",
    "    model3 = BiLSTM_train(X_seq_train_pad, y_train, embd_matrix, pretrained = False)\n",
    "    BiLSTM_test(model3, X_gen_seq_pad, y_gen)\n",
    "    \n",
    "    print(\"##### Bi-LSTM with pretraiend embeddings #####\")\n",
    "    model4 = BiLSTM_train(X_seq_train_pad, y_train, embd_matrix, pretrained = True)\n",
    "    BiLSTM_test(model4, X_gen_seq_pad, y_gen)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
